{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nguyen-nhat-mai/sentiment_analysis/blob/main/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nQJoXI4mu-X"
      },
      "source": [
        "## Environment set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htL75mplmb5R",
        "outputId": "00b9dc58-f7a6-4c48-c083-56e7c56b74fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.13.1\n",
            "  Downloading torch-1.13.1-cp39-cp39-manylinux1_x86_64.whl (887.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.4/887.4 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==1.13.1) (4.5.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (67.6.1)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (0.40.0)\n",
            "Installing collected packages: nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.0+cu118\n",
            "    Uninstalling torch-2.0.0+cu118:\n",
            "      Successfully uninstalled torch-2.0.0+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.15.1+cu118 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.15.1 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchdata 0.6.0 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchaudio 2.0.1+cu118 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 torch-1.13.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==1.13.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ns0NcQZnnJk2",
        "outputId": "db42e0aa-15b6-458d-ee48-a10f4253fca0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.13.1+cu117\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUigydiZrghW",
        "outputId": "c12ff5ec-313b-4bd7-ba42-28a8c1672b04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch-lightning==1.8.1\n",
            "  Downloading pytorch_lightning-1.8.1-py3-none-any.whl (798 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.4/798.4 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.8.1) (2023.4.0)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.8.1) (2.12.1)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.8.1) (1.22.4)\n",
            "Collecting torchmetrics>=0.7.0\n",
            "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lightning-utilities==0.3.*\n",
            "  Downloading lightning_utilities-0.3.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: torch>=1.9.* in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.8.1) (1.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.8.1) (4.5.0)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.8.1) (6.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.8.1) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.8.1) (4.65.0)\n",
            "Collecting fire\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning==1.8.1) (2.27.1)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.8.1) (1.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.8.1) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.8.1) (1.0.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.8.1) (0.7.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.8.1) (3.4.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.8.1) (67.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.8.1) (2.17.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.8.1) (2.2.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.8.1) (0.40.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.8.1) (1.53.0)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.8.1) (3.20.3)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch>=1.9.*->pytorch-lightning==1.8.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch>=1.9.*->pytorch-lightning==1.8.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.9/dist-packages (from torch>=1.9.*->pytorch-lightning==1.8.1) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.9/dist-packages (from torch>=1.9.*->pytorch-lightning==1.8.1) (8.5.0.96)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.8.1) (2.0.12)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning==1.8.1) (22.2.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning==1.8.1) (1.16.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning==1.8.1) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning==1.8.1) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning==1.8.1) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.9.1->pytorch-lightning==1.8.1) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard>=2.9.1->pytorch-lightning==1.8.1) (6.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning==1.8.1) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning==1.8.1) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning==1.8.1) (2022.12.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->pytorch-lightning==1.8.1) (2.1.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.9/dist-packages (from fire->lightning-utilities==0.3.*->pytorch-lightning==1.8.1) (2.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.9.1->pytorch-lightning==1.8.1) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning==1.8.1) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.9.1->pytorch-lightning==1.8.1) (3.2.2)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116952 sha256=80c8fddbb172a2dc738ca27cd83d255fec2933b47050f01e6af5ad471d360e2a\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/f1/89/b9ea2bf8f80ec027a88fef1d354b3816b4d3d29530988972f6\n",
            "Successfully built fire\n",
            "Installing collected packages: multidict, frozenlist, fire, async-timeout, yarl, lightning-utilities, aiosignal, aiohttp, torchmetrics, pytorch-lightning\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 fire-0.5.0 frozenlist-1.3.3 lightning-utilities-0.3.0 multidict-6.0.4 pytorch-lightning-1.8.1 torchmetrics-0.11.4 yarl-1.8.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.22.2\n",
            "  Downloading transformers-4.22.2-py3-none-any.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==4.22.2) (2.27.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.22.2) (23.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m102.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.9.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==4.22.2) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.22.2) (3.11.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.22.2) (1.22.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.22.2) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.22.2) (2022.10.31)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers==4.22.2) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.22.2) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.22.2) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.22.2) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.22.2) (2.0.12)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.4 tokenizers-0.12.1 transformers-4.22.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets==2.9.0\n",
            "  Downloading datasets-2.9.0-py3-none-any.whl (462 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.8/462.8 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.9/dist-packages (from datasets==2.9.0) (0.13.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets==2.9.0) (6.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets==2.9.0) (2.27.1)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets==2.9.0) (2023.4.0)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets==2.9.0) (23.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets==2.9.0) (9.0.0)\n",
            "Collecting dill<0.3.7\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets==2.9.0) (3.8.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets==2.9.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets==2.9.0) (1.22.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets==2.9.0) (1.5.3)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.9.0) (2.0.12)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.9.0) (1.8.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.9.0) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.9.0) (6.0.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.9.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.9.0) (22.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.9.0) (1.3.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.9.0) (3.11.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.9.0) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets==2.9.0) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets==2.9.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets==2.9.0) (2022.12.7)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets==2.9.0) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets==2.9.0) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets==2.9.0) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, responses, multiprocess, datasets\n",
            "Successfully installed datasets-2.9.0 dill-0.3.6 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece==0.1.97\n",
            "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scikit-learn==1.2.0\n",
            "  Downloading scikit_learn-1.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn==1.2.0) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn==1.2.0) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from scikit-learn==1.2.0) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn==1.2.0) (1.10.1)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "Successfully installed scikit-learn-1.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting numpy==1.23.5\n",
            "  Downloading numpy-1.23.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.15.1+cu118 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.15.1 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.5\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas==1.5.3 in /usr/local/lib/python3.9/dist-packages (1.5.3)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.9/dist-packages (from pandas==1.5.3) (1.23.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas==1.5.3) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas==1.5.3) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas==1.5.3) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.9/dist-packages (3.8.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk==3.8.1) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk==3.8.1) (4.65.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk==3.8.1) (8.1.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk==3.8.1) (2022.10.31)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting stanza==1.4.2\n",
            "  Downloading stanza-1.4.2-py3-none-any.whl (691 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m691.3/691.3 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from stanza==1.4.2) (2.27.1)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.9/dist-packages (from stanza==1.4.2) (1.13.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from stanza==1.4.2) (4.65.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.9/dist-packages (from stanza==1.4.2) (3.20.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from stanza==1.4.2) (1.23.5)\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.2.0.tar.gz (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.9/240.9 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from stanza==1.4.2) (1.16.0)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->stanza==1.4.2) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->stanza==1.4.2) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->stanza==1.4.2) (11.7.99)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->stanza==1.4.2) (4.5.0)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->stanza==1.4.2) (8.5.0.96)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.3.0->stanza==1.4.2) (67.6.1)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.3.0->stanza==1.4.2) (0.40.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->stanza==1.4.2) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->stanza==1.4.2) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->stanza==1.4.2) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->stanza==1.4.2) (2.0.12)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.2.0-py3-none-any.whl size=234926 sha256=ca486e7c6d7955404cc2e8f0b7d04baa318aef2ccb91dd3d5eb97edf896fcf44\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/b8/0f/f580817231cbf59f6ade9fd132ff60ada1de9f7dc85521f857\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji, stanza\n",
            "Successfully installed emoji-2.2.0 stanza-1.4.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-lightning==1.8.1\n",
        "!pip install transformers==4.22.2\n",
        "!pip install datasets==2.9.0\n",
        "!pip install sentencepiece==0.1.97\n",
        "!pip install scikit-learn==1.2.0\n",
        "!pip install numpy==1.23.5\n",
        "!pip install pandas==1.5.3\n",
        "!pip install nltk==3.8.1\n",
        "!pip install stanza==1.4.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from transformers import BertModel, BertTokenizer\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.nn.functional as F\n",
        "import random as rn\n",
        "from random import shuffle\n",
        "import time, sys\n",
        "np.random.seed(17)\n",
        "rn.seed(12345)"
      ],
      "metadata": {
        "id": "7aXWFkIq6l3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fN6fMzOyrvvF"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQksKfGELBV7"
      },
      "outputs": [],
      "source": [
        "# Network\n",
        "class SentimentCheck(nn.Module):\n",
        "    \n",
        "    def __init__(self, n_classes=3):\n",
        "        super(SentimentCheck, self).__init__()\n",
        "        self.model = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.drop = nn.Dropout(p=0.45)\n",
        "        self.fc = nn.Linear(self.model.config.hidden_size, n_classes)\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        _, output = self.model(input_ids = input_ids, attention_mask = attention_mask)[0:2]\n",
        "        output = self.drop(output)\n",
        "        output = self.fc(output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UI997K2rLGGF"
      },
      "outputs": [],
      "source": [
        "# Preprocessing\n",
        "\n",
        "#----------------PREPROCESS RAW DATA----------------#\n",
        "# Converting aspects to questions and consider sentences as replies (account for context)\n",
        "\n",
        "aspect_map = {\"AMBIENCE#GENERAL\":\"What do you think of the ambience ?\",\n",
        "              \"FOOD#QUALITY\":\"What do you think of the quality of the food ?\",\n",
        "              \"SERVICE#GENERAL\":\"What do you think of the service ?\",\n",
        "              \"FOOD#STYLE_OPTIONS\": \"What do you think of the food choices ?\",\n",
        "              \"DRINKS#QUALITY\":\"What do you think of the drinks?\",\n",
        "              'RESTAURANT#MISCELLANEOUS': \"What do you think of the restaurant ?\",\n",
        "              'RESTAURANT#GENERAL': \"What do you think of the restaurant ?\",\n",
        "              'LOCATION#GENERAL': 'What do you think of the location ?',\n",
        "              'DRINKS#STYLE_OPTIONS': \"What do you think of the drink choices ?\",\n",
        "              'RESTAURANT#PRICES':'What do you think of the price of it ?',\n",
        "              'DRINKS#PRICES':'What do you think of the price of it ?',\n",
        "              'FOOD#PRICES': 'What do you think of the price of it ?'            \n",
        "              }\n",
        "label_encode = {\"negative\":0,\"neutral\":1,\"positive\":2}\n",
        "\n",
        "def preprocessing(file):\n",
        "    file[0] = file[0].apply(lambda x: label_encode[x])\n",
        "    file[1] = file[1].apply(lambda x: aspect_map[x])\n",
        "    return file\n",
        "\n",
        "#----------------PREPARE PYTORCH DATASET----------------#\n",
        "# Function to calculate max token length as input for creating pytorch dataset\n",
        "def max_len_token(file, tokenizer):\n",
        "    token_lengths = []\n",
        "    for sentence in file[4]:\n",
        "        tokens = tokenizer.encode(sentence, max_length=1000)\n",
        "        token_lengths.append(len(tokens))\n",
        "    return max(token_lengths)\n",
        "\n",
        "# Create pytorch dataset\n",
        "class create_dataset(Dataset):\n",
        "    \n",
        "    def __init__(self, sentences, aspect_categories, target_terms, labels, tokenizer, max_len_token):\n",
        "        \n",
        "        self.sentences = sentences\n",
        "        self.aspect_categories = aspect_categories\n",
        "        self.target_terms = target_terms\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len_token = max_len_token\n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        sentence = str(self.sentences[index])\n",
        "        aspect_category = str(self.aspect_categories[index])\n",
        "        target_term = str(self.target_terms[index])\n",
        "        label = self.labels[index]\n",
        "\n",
        "        aspect = aspect_category + ' ' + target_term\n",
        "        \n",
        "        # Get the attention maks\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text = sentence,\n",
        "            text_pair = aspect,\n",
        "            add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
        "            max_length=self.max_len_token, # Pad & truncate all sentences.\n",
        "            return_token_type_ids=False,\n",
        "            pad_to_max_length=True, # Pad to the end of the sentence\n",
        "            return_attention_mask=True, # Construct attn. masks.\n",
        "            return_tensors='pt',  # Return PyTorch tensors\n",
        "            truncation=True)\n",
        "\n",
        "        return {\n",
        "            'sentence_text': sentence,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "im0VCQLZLKx1"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "class Classifier:\n",
        "    \"\"\"\n",
        "    The Classifier: complete the definition of this class template by providing a constructor (i.e. the\n",
        "    __init__() function) and the 2 methods train() and predict() below. Please donot change\n",
        "     \"\"\"\n",
        "\n",
        "    ###############################################################################\n",
        "    def train(self, train_filename: str, dev_filename: str, device: torch.device):\n",
        "        \"\"\"\n",
        "        Trains the classifier model on the training set stored in file trainfile\n",
        "        PLEASE:\n",
        "          - DO NOT CHANGE THE SIGNATURE OF THIS METHOD\n",
        "          - PUT THE MODEL and DATA on the specified device! Do not use another device\n",
        "          - DO NOT USE THE DEV DATA AS TRAINING EXAMPLES, YOU CAN USE THEM ONLY FOR THE OPTIMIZATION\n",
        "         OF MODEL HYPERPARAMETERS\n",
        "        \"\"\"\n",
        "        \n",
        "        #----------------LOAD FILE & PREPROCESS----------------#\n",
        "        # Load the files into pandas dataframe\n",
        "        train_file = pd.read_csv(train_filename, sep='\\t', header=None)\n",
        "        if dev_filename is not None:\n",
        "            dev_file = pd.read_csv(dev_filename, sep='\\t', header=None)\n",
        "        \n",
        "        # Proprocess the file\n",
        "        self.trainfile = preprocessing(train_file)\n",
        "        if dev_filename is not None:\n",
        "            self.dev_file = preprocessing(dev_file)\n",
        "        \n",
        "        #----------------ENCODING & GET DATALOADER------------------#\n",
        "        # Set the pretrained model & tokenizing\n",
        "        PRE_TRAINED_MODEL = 'bert-base-uncased'\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL)\n",
        "        \n",
        "        # Load the dataloader\n",
        "        self.bs = 16 # Set batch size\n",
        "        self.max_length = max_len_token(self.trainfile, self.tokenizer)\n",
        "        self.train_dataset = create_dataset(\n",
        "            sentences=self.trainfile[4].to_numpy(),\n",
        "            aspect_categories = self.trainfile[1].to_numpy(),\n",
        "            target_terms=self.trainfile[2].to_numpy(),\n",
        "            labels = self.trainfile[0].to_numpy(),\n",
        "            tokenizer = self.tokenizer,\n",
        "            max_len_token = self.max_length)\n",
        "        self.train_loader = DataLoader(self.train_dataset, batch_size= self.bs, shuffle=True)\n",
        "        if dev_filename is not None: \n",
        "            self.dev_dataset = create_dataset(\n",
        "                sentences=self.dev_file[4].to_numpy(),\n",
        "                aspect_categories = self.dev_file[1].to_numpy(),\n",
        "                target_terms=self.dev_file[2].to_numpy(),\n",
        "                labels = self.dev_file[0].to_numpy(),\n",
        "                tokenizer = self.tokenizer,\n",
        "                max_len_token = self.max_length)\n",
        "            self.val_loader = DataLoader(self.dev_dataset, batch_size= self.bs, shuffle=True)\n",
        "            \n",
        "        #----------------MODEL & HYPEPARAMETERS------------------#\n",
        "        # Define the devide and the model\n",
        "        self.device = device\n",
        "        self.model = SentimentCheck().to(self.device)\n",
        "    \n",
        "        # Hyperparameters\n",
        "        self.epochs = 15\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=2e-5)\n",
        "        self.total_steps = len(self.train_loader) * self.epochs\n",
        "        self.scheduler = lr_scheduler.LinearLR(self.optimizer, total_iters=self.total_steps) # reduce lr gradually to 0\n",
        "        self.loss_fn = nn.CrossEntropyLoss().to(self.device)\n",
        "               \n",
        "        #----------------TRAIN & VALIDATE MODEL------------------#\n",
        "        record = defaultdict(list)\n",
        "        best_val_acc = 0\n",
        "        for epoch in tqdm(range(self.epochs)):\n",
        "            print()\n",
        "            print('=' * 10, f'Epoch {epoch + 1}/{self.epochs}', '=' * 10)\n",
        "            print()\n",
        "            \n",
        "            #----------------TRAINING------------------#\n",
        "            self.model.train()\n",
        "            losses = []\n",
        "            correct_predictions = 0\n",
        "            \n",
        "            for batch in self.train_loader:\n",
        "                # Get inputs for model\n",
        "                input_ids = batch[\"input_ids\"].to(self.device)\n",
        "                attention_mask = batch[\"attention_mask\"].to(self.device)\n",
        "                labels = batch[\"labels\"].to(self.device)\n",
        "                # Predict and calculate loss\n",
        "                outputs = self.model(input_ids = input_ids, attention_mask = attention_mask)\n",
        "                _, preds = torch.max(outputs, dim=1)\n",
        "                loss = self.loss_fn(outputs, labels)\n",
        "                correct_predictions += torch.sum(preds == labels)\n",
        "                losses.append(loss.item())\n",
        "                # Backprop\n",
        "                loss.backward()\n",
        "                # Gradient clipping\n",
        "                nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "                # Take next step\n",
        "                self.optimizer.step()\n",
        "                self.scheduler.step()\n",
        "                \n",
        "            train_acc, train_loss = correct_predictions.double() / len(self.trainfile), np.mean(losses)        \n",
        "            print(f'Train loss {train_loss} - Training accuracy {train_acc}')\n",
        "            print()\n",
        "            \n",
        "            #----------------VALIDATION------------------#\n",
        "            if self.val_loader is not None:\n",
        "                self.model = self.model.eval()    \n",
        "                losses = []\n",
        "                correct_predictions = 0\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                    for batch in self.val_loader:\n",
        "                        # Get inputs for model\n",
        "                        input_ids = batch[\"input_ids\"].to(self.device)\n",
        "                        attention_mask = batch[\"attention_mask\"].to(self.device)\n",
        "                        labels = batch[\"labels\"].to(self.device)\n",
        "                        # Predict and calculate loss\n",
        "                        outputs = self.model(input_ids = input_ids, attention_mask = attention_mask)\n",
        "                        _, preds = torch.max(outputs, dim=1)\n",
        "                        loss = self.loss_fn(outputs, labels)\n",
        "                        correct_predictions += torch.sum(preds == labels)\n",
        "                        losses.append(loss.item())\n",
        "                        \n",
        "                val_acc, val_loss = correct_predictions.double() / len(self.dev_file), np.mean(losses)\n",
        "                print(f'Val loss {val_loss} - Val accuracy {val_acc}')\n",
        "                print()\n",
        "                \n",
        "            #----------------RECORD ACCURACY & SAVE BEST MODEL------------------#\n",
        "            record['train_acc'].append(train_acc)\n",
        "            record['train_loss'].append(train_loss)\n",
        "                \n",
        "            if self.val_loader is not None:\n",
        "                record['val_acc'].append(val_acc)\n",
        "                record['val_loss'].append(val_loss)\n",
        "                \n",
        "            if (self.val_loader is not None) and (val_acc > best_val_acc):\n",
        "                torch.save(self.model.state_dict(), 'best_model.pth')\n",
        "                best_val_acc = val_acc\n",
        "        \n",
        "    \n",
        "    ###############################################################################\n",
        "    def predict(self, data_filename: str, device: torch.device):\n",
        "        \"\"\"Predicts class labels for the input instances in file 'datafile'\n",
        "        Returns the list of predicted labels\n",
        "        PLEASE:\n",
        "          - DO NOT CHANGE THE SIGNATURE OF THIS METHOD\n",
        "          - PUT THE MODEL and DATA on the specified device! Do not use another device\n",
        "        \"\"\"\n",
        "        \n",
        "        #----------------LOAD FILE & PREPROCESS----------------#\n",
        "        data_file = pd.read_csv(data_filename, sep='\\t', header=None)\n",
        "        self.data_file = preprocessing(data_file)\n",
        "        self.dataset = create_dataset(\n",
        "            sentences=self.data_file[4].to_numpy(),\n",
        "            aspect_categories = self.data_file[1].to_numpy(),\n",
        "            target_terms=self.data_file[2].to_numpy(),\n",
        "            labels = self.data_file[0].to_numpy(),\n",
        "            tokenizer = self.tokenizer,\n",
        "            max_len_token = self.max_length)\n",
        "        self.data_loader = DataLoader(self.dataset, batch_size= self.bs, shuffle=False)\n",
        "        \n",
        "        #----------------LOAD MODEL------------------#\n",
        "        self.device = device\n",
        "        self.model = SentimentCheck().to(self.device)\n",
        "        self.model.load_state_dict(torch.load('best_model.pth'))\n",
        "\n",
        "        #----------------PREDICTING------------------#\n",
        "        self.model.eval()\n",
        "        output_labels = []\n",
        "        label_decode = {0:\"negative\",1:\"neutral\",2:\"positive\"}\n",
        "        for batch in self.data_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(self.device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(self.device)\n",
        "            labels = batch[\"labels\"].to(self.device)\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(input_ids=input_ids,attention_mask=attention_mask)\n",
        "            logits = F.softmax(outputs, dim=1)\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            outputs = np.argmax(logits, axis=1) # label in number 0,1,2\n",
        "            for label in outputs:\n",
        "                output_labels.append(label_decode[label]) # label in text \"negative\",\"neutral\",\"positive\"\n",
        "        return np.array(output_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoOG4Q8i9beC",
        "outputId": "ba946a2e-d5f6-40a4-dfd2-932d95999c28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "../data/traindata.csv\n",
            "\n",
            "RUN: 1\n",
            "  1.1. Training the classifier...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "  0%|          | 0/15 [00:00<?, ?it/s]/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2302: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== Epoch 1/15 ==========\n",
            "\n",
            "Train loss 0.7451464720862977 - Training accuracy 0.7059214903526281\n",
            "\n",
            "Val loss 0.5394221792618433 - Val accuracy 0.8138297872340425\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 1/15 [00:12<02:52, 12.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== Epoch 2/15 ==========\n",
            "\n",
            "Train loss 0.45461806440924074 - Training accuracy 0.8496340652029275\n",
            "\n",
            "Val loss 0.3936898075044155 - Val accuracy 0.8563829787234042\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 2/15 [00:24<02:40, 12.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== Epoch 3/15 ==========\n",
            "\n",
            "Train loss 0.3376840125531592 - Training accuracy 0.8848968729208251\n",
            "\n",
            "Val loss 0.4013814491530259 - Val accuracy 0.8696808510638298\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 3/15 [00:36<02:27, 12.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== Epoch 4/15 ==========\n",
            "\n",
            "Train loss 0.28719070864880974 - Training accuracy 0.9108449767132402\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 27%|██▋       | 4/15 [00:48<02:11, 11.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.5534217798461517 - Val accuracy 0.7872340425531915\n",
            "\n",
            "\n",
            "========== Epoch 5/15 ==========\n",
            "\n",
            "Train loss 0.2559786610543094 - Training accuracy 0.9161676646706587\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 5/15 [00:59<01:57, 11.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.5120654584219059 - Val accuracy 0.8484042553191489\n",
            "\n",
            "\n",
            "========== Epoch 6/15 ==========\n",
            "\n",
            "Train loss 0.19095594022779705 - Training accuracy 0.9341317365269461\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 6/15 [01:11<01:44, 11.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.5704893353395164 - Val accuracy 0.8484042553191489\n",
            "\n",
            "\n",
            "========== Epoch 7/15 ==========\n",
            "\n",
            "Train loss 0.1644676976609341 - Training accuracy 0.9500998003992016\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 47%|████▋     | 7/15 [01:22<01:32, 11.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.5759294821570317 - Val accuracy 0.8537234042553191\n",
            "\n",
            "\n",
            "========== Epoch 8/15 ==========\n",
            "\n",
            "Train loss 0.15034985727075725 - Training accuracy 0.957418496340652\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 53%|█████▎    | 8/15 [01:33<01:20, 11.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.7327674117404968 - Val accuracy 0.8484042553191489\n",
            "\n",
            "\n",
            "========== Epoch 9/15 ==========\n",
            "\n",
            "Train loss 0.13353128639356968 - Training accuracy 0.9627411842980705\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 9/15 [01:45<01:08, 11.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.6731476188870147 - Val accuracy 0.8590425531914894\n",
            "\n",
            "\n",
            "========== Epoch 10/15 ==========\n",
            "\n",
            "Train loss 0.10937740854012404 - Training accuracy 0.9654025282767797\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 10/15 [01:56<00:57, 11.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.7289829888225844 - Val accuracy 0.8537234042553191\n",
            "\n",
            "\n",
            "========== Epoch 11/15 ==========\n",
            "\n",
            "Train loss 0.14886896508347064 - Training accuracy 0.9654025282767797\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 73%|███████▎  | 11/15 [02:07<00:45, 11.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 1.1120558294157188 - Val accuracy 0.8031914893617021\n",
            "\n",
            "\n",
            "========== Epoch 12/15 ==========\n",
            "\n",
            "Train loss 0.11779867312152613 - Training accuracy 0.9687292082501664\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 12/15 [02:19<00:34, 11.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.9247748407894202 - Val accuracy 0.8537234042553191\n",
            "\n",
            "\n",
            "========== Epoch 13/15 ==========\n",
            "\n",
            "Train loss 0.0924578438150112 - Training accuracy 0.9747172322022621\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 87%|████████▋ | 13/15 [02:30<00:22, 11.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.8362694581761995 - Val accuracy 0.8404255319148936\n",
            "\n",
            "\n",
            "========== Epoch 14/15 ==========\n",
            "\n",
            "Train loss 0.06420573520479089 - Training accuracy 0.9820359281437125\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 93%|█████████▎| 14/15 [02:41<00:11, 11.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.6668816726693573 - Val accuracy 0.8484042553191489\n",
            "\n",
            "\n",
            "========== Epoch 15/15 ==========\n",
            "\n",
            "Train loss 0.07600634237454135 - Training accuracy 0.9820359281437125\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [02:53<00:00, 11.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.8271216926708197 - Val accuracy 0.8537234042553191\n",
            "\n",
            "\n",
            "  1.2. Eval on the dev set..."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2302: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Acc.: 86.97\n",
            "\n",
            "\n",
            "RUN: 2\n",
            "  2.1. Training the classifier...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "  0%|          | 0/15 [00:00<?, ?it/s]/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2302: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== Epoch 1/15 ==========\n",
            "\n",
            "Train loss 0.6711811386841409 - Training accuracy 0.7511643379906853\n",
            "\n",
            "Val loss 0.5067118139316639 - Val accuracy 0.8085106382978723\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 1/15 [00:12<02:53, 12.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== Epoch 2/15 ==========\n",
            "\n",
            "Train loss 0.42462872674173496 - Training accuracy 0.8689288090485695\n",
            "\n",
            "Val loss 0.4494043073306481 - Val accuracy 0.8457446808510638\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 2/15 [00:24<02:39, 12.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== Epoch 3/15 ==========\n",
            "\n",
            "Train loss 0.35090896578386743 - Training accuracy 0.888888888888889\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 3/15 [00:35<02:22, 11.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.47262899205088615 - Val accuracy 0.8351063829787234\n",
            "\n",
            "\n",
            "========== Epoch 4/15 ==========\n",
            "\n",
            "Train loss 0.3017202211900594 - Training accuracy 0.9055222887558217\n",
            "\n",
            "Val loss 0.4978406199564536 - Val accuracy 0.8484042553191489\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 27%|██▋       | 4/15 [00:48<02:12, 12.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== Epoch 5/15 ==========\n",
            "\n",
            "Train loss 0.276034267024791 - Training accuracy 0.9155023286759814\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 5/15 [00:59<01:57, 11.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.5635853223502636 - Val accuracy 0.8271276595744681\n",
            "\n",
            "\n",
            "========== Epoch 6/15 ==========\n",
            "\n",
            "Train loss 0.22944982252777257 - Training accuracy 0.9248170326014638\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 6/15 [01:10<01:44, 11.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.5527800557514032 - Val accuracy 0.8351063829787234\n",
            "\n",
            "\n",
            "========== Epoch 7/15 ==========\n",
            "\n",
            "Train loss 0.20063869242972515 - Training accuracy 0.9308050565535596\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 47%|████▋     | 7/15 [01:22<01:32, 11.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.6294241192129751 - Val accuracy 0.8297872340425532\n",
            "\n",
            "\n",
            "========== Epoch 8/15 ==========\n",
            "\n",
            "Train loss 0.1530348434400289 - Training accuracy 0.9520958083832336\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 53%|█████▎    | 8/15 [01:33<01:20, 11.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.6079216750028232 - Val accuracy 0.8430851063829787\n",
            "\n",
            "\n",
            "========== Epoch 9/15 ==========\n",
            "\n",
            "Train loss 0.1478626601883468 - Training accuracy 0.957418496340652\n",
            "\n",
            "Val loss 0.622757780055205 - Val accuracy 0.8563829787234042\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 9/15 [01:45<01:10, 11.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== Epoch 10/15 ==========\n",
            "\n",
            "Train loss 0.11269146341216216 - Training accuracy 0.9673985362608117\n",
            "\n",
            "Val loss 0.5803188468174388 - Val accuracy 0.8590425531914894\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 10/15 [01:58<00:59, 11.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== Epoch 11/15 ==========\n",
            "\n",
            "Train loss 0.11610132248012071 - Training accuracy 0.9660678642714571\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 73%|███████▎  | 11/15 [02:09<00:46, 11.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.7397136048724254 - Val accuracy 0.8430851063829787\n",
            "\n",
            "\n",
            "========== Epoch 12/15 ==========\n",
            "\n",
            "Train loss 0.09736690308745118 - Training accuracy 0.9727212242182303\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 12/15 [02:20<00:34, 11.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.855380897099773 - Val accuracy 0.8537234042553191\n",
            "\n",
            "\n",
            "========== Epoch 13/15 ==========\n",
            "\n",
            "Train loss 0.11455290974241662 - Training accuracy 0.9713905522288756\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 87%|████████▋ | 13/15 [02:32<00:23, 11.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.8477886735927314 - Val accuracy 0.8430851063829787\n",
            "\n",
            "\n",
            "========== Epoch 14/15 ==========\n",
            "\n",
            "Train loss 0.10399122822204941 - Training accuracy 0.9740518962075848\n",
            "\n",
            "Val loss 0.8090848625724902 - Val accuracy 0.8643617021276595\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 93%|█████████▎| 14/15 [02:44<00:11, 11.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== Epoch 15/15 ==========\n",
            "\n",
            "Train loss 0.07842103346805782 - Training accuracy 0.9773785761809715\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [02:55<00:00, 11.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.8865578463883139 - Val accuracy 0.8537234042553191\n",
            "\n",
            "\n",
            "  2.2. Eval on the dev set..."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2302: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Acc.: 86.44\n",
            "\n",
            "\n",
            "RUN: 3\n",
            "  3.1. Training the classifier...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "  0%|          | 0/15 [00:00<?, ?it/s]/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2302: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== Epoch 1/15 ==========\n",
            "\n",
            "Train loss 0.678602361615668 - Training accuracy 0.7458416500332669\n",
            "\n",
            "Val loss 0.4596155422429244 - Val accuracy 0.8457446808510638\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 1/15 [00:12<02:52, 12.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== Epoch 2/15 ==========\n",
            "\n",
            "Train loss 0.4187905813785309 - Training accuracy 0.8709248170326015\n",
            "\n",
            "Val loss 0.44560410113384324 - Val accuracy 0.8484042553191489\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 2/15 [00:24<02:39, 12.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== Epoch 3/15 ==========\n",
            "\n",
            "Train loss 0.35063146038892423 - Training accuracy 0.8915502328675982\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 3/15 [00:35<02:22, 11.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.45867781682560843 - Val accuracy 0.8377659574468085\n",
            "\n",
            "\n",
            "========== Epoch 4/15 ==========\n",
            "\n",
            "Train loss 0.30729878975197356 - Training accuracy 0.908183632734531\n",
            "\n",
            "Val loss 0.4842602907059093 - Val accuracy 0.851063829787234\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 27%|██▋       | 4/15 [00:48<02:12, 12.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== Epoch 5/15 ==========\n",
            "\n",
            "Train loss 0.26998945892332715 - Training accuracy 0.9155023286759814\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 5/15 [00:59<01:57, 11.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.5610004474098483 - Val accuracy 0.8404255319148936\n",
            "\n",
            "\n",
            "========== Epoch 6/15 ==========\n",
            "\n",
            "Train loss 0.236819463890997 - Training accuracy 0.9254823685961411\n",
            "\n",
            "Val loss 0.5654406735363106 - Val accuracy 0.8563829787234042\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 6/15 [01:11<01:47, 11.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== Epoch 7/15 ==========\n",
            "\n",
            "Train loss 0.20048869868859331 - Training accuracy 0.9394544244843647\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 47%|████▋     | 7/15 [01:23<01:33, 11.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.5649616441999873 - Val accuracy 0.8457446808510638\n",
            "\n",
            "\n",
            "========== Epoch 8/15 ==========\n",
            "\n",
            "Train loss 0.1521600273913367 - Training accuracy 0.9540918163672655\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 53%|█████▎    | 8/15 [01:34<01:21, 11.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.624512090968589 - Val accuracy 0.8563829787234042\n",
            "\n",
            "\n",
            "========== Epoch 9/15 ==========\n",
            "\n",
            "Train loss 0.15278496605621533 - Training accuracy 0.9580838323353293\n",
            "\n",
            "Val loss 0.5789396949888518 - Val accuracy 0.8670212765957447\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 9/15 [01:46<01:10, 11.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== Epoch 10/15 ==========\n",
            "\n",
            "Train loss 0.1352622543803436 - Training accuracy 0.9627411842980705\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 10/15 [01:58<00:58, 11.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.801596310241924 - Val accuracy 0.8457446808510638\n",
            "\n",
            "\n",
            "========== Epoch 11/15 ==========\n",
            "\n",
            "Train loss 0.13469955928752475 - Training accuracy 0.9620758483033932\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 73%|███████▎  | 11/15 [02:09<00:46, 11.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.8539499013374249 - Val accuracy 0.8430851063829787\n",
            "\n",
            "\n",
            "========== Epoch 12/15 ==========\n",
            "\n",
            "Train loss 0.12588786179363293 - Training accuracy 0.9667332002661344\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 12/15 [02:20<00:34, 11.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.7461864253079208 - Val accuracy 0.8537234042553191\n",
            "\n",
            "\n",
            "========== Epoch 13/15 ==========\n",
            "\n",
            "Train loss 0.12555925369738263 - Training accuracy 0.9693945442448436\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 87%|████████▋ | 13/15 [02:32<00:22, 11.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.7981744071294088 - Val accuracy 0.851063829787234\n",
            "\n",
            "\n",
            "========== Epoch 14/15 ==========\n",
            "\n",
            "Train loss 0.11080411756832272 - Training accuracy 0.9760479041916168\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 93%|█████████▎| 14/15 [02:43<00:11, 11.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.8241718835003363 - Val accuracy 0.851063829787234\n",
            "\n",
            "\n",
            "========== Epoch 15/15 ==========\n",
            "\n",
            "Train loss 0.11938948921507542 - Training accuracy 0.9753825681969395\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [02:54<00:00, 11.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.7836572881496977 - Val accuracy 0.8590425531914894\n",
            "\n",
            "\n",
            "  3.2. Eval on the dev set..."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2302: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Acc.: 86.70\n",
            "\n",
            "\n",
            "RUN: 4\n",
            "  4.1. Training the classifier...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "  0%|          | 0/15 [00:00<?, ?it/s]/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2302: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== Epoch 1/15 ==========\n",
            "\n",
            "Train loss 0.7469641661390345 - Training accuracy 0.7005988023952096\n",
            "\n",
            "Val loss 0.5071849388380846 - Val accuracy 0.8324468085106382\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 1/15 [00:12<02:52, 12.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== Epoch 2/15 ==========\n",
            "\n",
            "Train loss 0.43564521275619245 - Training accuracy 0.8449767132401863\n",
            "\n",
            "Val loss 0.43534870631992817 - Val accuracy 0.8377659574468085\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 2/15 [00:24<02:39, 12.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== Epoch 3/15 ==========\n",
            "\n",
            "Train loss 0.33019673313073655 - Training accuracy 0.8942115768463074\n",
            "\n",
            "Val loss 0.45334448851644993 - Val accuracy 0.8723404255319148\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 3/15 [00:36<02:27, 12.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== Epoch 4/15 ==========\n",
            "\n",
            "Train loss 0.2767781622390798 - Training accuracy 0.9041916167664671\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 27%|██▋       | 4/15 [00:48<02:11, 11.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.49637737528731424 - Val accuracy 0.8484042553191489\n",
            "\n",
            "\n",
            "========== Epoch 5/15 ==========\n",
            "\n",
            "Train loss 0.228837531674257 - Training accuracy 0.9261477045908184\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 5/15 [00:59<01:57, 11.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.5728680519387126 - Val accuracy 0.8457446808510638\n",
            "\n",
            "\n",
            "========== Epoch 6/15 ==========\n",
            "\n",
            "Train loss 0.21832916891559007 - Training accuracy 0.936127744510978\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 6/15 [01:10<01:44, 11.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.47288419826266664 - Val accuracy 0.8457446808510638\n",
            "\n",
            "\n",
            "========== Epoch 7/15 ==========\n",
            "\n",
            "Train loss 0.17187477390162964 - Training accuracy 0.9467731204258151\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 47%|████▋     | 7/15 [01:22<01:31, 11.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.5288364583199533 - Val accuracy 0.8484042553191489\n",
            "\n",
            "\n",
            "========== Epoch 8/15 ==========\n",
            "\n",
            "Train loss 0.14391735519104182 - Training accuracy 0.959414504324684\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 53%|█████▎    | 8/15 [01:33<01:20, 11.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.5921021970376993 - Val accuracy 0.851063829787234\n",
            "\n",
            "\n",
            "========== Epoch 9/15 ==========\n",
            "\n",
            "Train loss 0.11675339831940909 - Training accuracy 0.9654025282767797\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 9/15 [01:44<01:08, 11.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.8102335308988889 - Val accuracy 0.8297872340425532\n",
            "\n",
            "\n",
            "========== Epoch 10/15 ==========\n",
            "\n",
            "Train loss 0.11085688747946133 - Training accuracy 0.9680638722554891\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 10/15 [01:56<00:56, 11.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.8408653390749047 - Val accuracy 0.8430851063829787\n",
            "\n",
            "\n",
            "========== Epoch 11/15 ==========\n",
            "\n",
            "Train loss 0.10592497992477914 - Training accuracy 0.9727212242182303\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 73%|███████▎  | 11/15 [02:07<00:45, 11.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.9251106303369548 - Val accuracy 0.8484042553191489\n",
            "\n",
            "\n",
            "========== Epoch 12/15 ==========\n",
            "\n",
            "Train loss 0.1550832996454506 - Training accuracy 0.9667332002661344\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 12/15 [02:18<00:34, 11.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.822391099393523 - Val accuracy 0.8457446808510638\n",
            "\n",
            "\n",
            "========== Epoch 13/15 ==========\n",
            "\n",
            "Train loss 0.09592232049784642 - Training accuracy 0.9747172322022621\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 87%|████████▋ | 13/15 [02:30<00:22, 11.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.9517057842798143 - Val accuracy 0.8244680851063829\n",
            "\n",
            "\n",
            "========== Epoch 14/15 ==========\n",
            "\n",
            "Train loss 0.10816941365285063 - Training accuracy 0.9740518962075848\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 93%|█████████▎| 14/15 [02:41<00:11, 11.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.7783531164944483 - Val accuracy 0.8457446808510638\n",
            "\n",
            "\n",
            "========== Epoch 15/15 ==========\n",
            "\n",
            "Train loss 0.10448789904129374 - Training accuracy 0.9747172322022621\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [02:52<00:00, 11.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 1.0475289700552821 - Val accuracy 0.8297872340425532\n",
            "\n",
            "\n",
            "  4.2. Eval on the dev set..."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2302: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Acc.: 87.23\n",
            "\n",
            "\n",
            "RUN: 5\n",
            "  5.1. Training the classifier...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "  0%|          | 0/15 [00:00<?, ?it/s]/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2302: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== Epoch 1/15 ==========\n",
            "\n",
            "Train loss 0.7026962216230149 - Training accuracy 0.7232202262142382\n",
            "\n",
            "Val loss 0.44756957453986007 - Val accuracy 0.8457446808510638\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 1/15 [00:12<02:52, 12.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== Epoch 2/15 ==========\n",
            "\n",
            "Train loss 0.40509676188230515 - Training accuracy 0.8616101131071191\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 2/15 [00:23<02:32, 11.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.4101928590486447 - Val accuracy 0.8404255319148936\n",
            "\n",
            "\n",
            "========== Epoch 3/15 ==========\n",
            "\n",
            "Train loss 0.31542681997760813 - Training accuracy 0.9001996007984032\n",
            "\n",
            "Val loss 0.4050348731689155 - Val accuracy 0.8882978723404256\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 3/15 [00:35<02:23, 11.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== Epoch 4/15 ==========\n",
            "\n",
            "Train loss 0.26846448338015916 - Training accuracy 0.9115103127079175\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 27%|██▋       | 4/15 [00:47<02:09, 11.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.44008215943661827 - Val accuracy 0.8590425531914894\n",
            "\n",
            "\n",
            "========== Epoch 5/15 ==========\n",
            "\n",
            "Train loss 0.20949657570491445 - Training accuracy 0.927478376580173\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 5/15 [00:58<01:55, 11.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.6285092948625485 - Val accuracy 0.8430851063829787\n",
            "\n",
            "\n",
            "========== Epoch 6/15 ==========\n",
            "\n",
            "Train loss 0.20875765971760166 - Training accuracy 0.936127744510978\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 6/15 [01:09<01:43, 11.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.6026292233727872 - Val accuracy 0.8377659574468085\n",
            "\n",
            "\n",
            "========== Epoch 7/15 ==========\n",
            "\n",
            "Train loss 0.17055669174767396 - Training accuracy 0.9534264803725881\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 47%|████▋     | 7/15 [01:21<01:31, 11.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.5953699523427834 - Val accuracy 0.8484042553191489\n",
            "\n",
            "\n",
            "========== Epoch 8/15 ==========\n",
            "\n",
            "Train loss 0.13580399049951278 - Training accuracy 0.9620758483033932\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 53%|█████▎    | 8/15 [01:32<01:19, 11.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.5355156695780655 - Val accuracy 0.875\n",
            "\n",
            "\n",
            "========== Epoch 9/15 ==========\n",
            "\n",
            "Train loss 0.10505442375049709 - Training accuracy 0.9687292082501664\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 9/15 [01:43<01:08, 11.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.6582490552294379 - Val accuracy 0.8590425531914894\n",
            "\n",
            "\n",
            "========== Epoch 10/15 ==========\n",
            "\n",
            "Train loss 0.1002427960691498 - Training accuracy 0.9727212242182303\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 10/15 [01:55<00:56, 11.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.721562508008598 - Val accuracy 0.8643617021276595\n",
            "\n",
            "\n",
            "========== Epoch 11/15 ==========\n",
            "\n",
            "Train loss 0.08360814849197111 - Training accuracy 0.9740518962075848\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 73%|███████▎  | 11/15 [02:06<00:45, 11.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 1.0508884990413208 - Val accuracy 0.7845744680851063\n",
            "\n",
            "\n",
            "========== Epoch 12/15 ==========\n",
            "\n",
            "Train loss 0.10950984047822397 - Training accuracy 0.9740518962075848\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 12/15 [02:17<00:34, 11.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.857340856838467 - Val accuracy 0.8404255319148936\n",
            "\n",
            "\n",
            "========== Epoch 13/15 ==========\n",
            "\n",
            "Train loss 0.13825690258168022 - Training accuracy 0.9647371922821025\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 87%|████████▋ | 13/15 [02:29<00:22, 11.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.726646188646555 - Val accuracy 0.8324468085106382\n",
            "\n",
            "\n",
            "========== Epoch 14/15 ==========\n",
            "\n",
            "Train loss 0.10091370135419407 - Training accuracy 0.9767132401862941\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 93%|█████████▎| 14/15 [02:40<00:11, 11.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 0.8733855766137518 - Val accuracy 0.8457446808510638\n",
            "\n",
            "\n",
            "========== Epoch 15/15 ==========\n",
            "\n",
            "Train loss 0.11742894566541122 - Training accuracy 0.9753825681969395\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [02:52<00:00, 11.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss 1.058729358536463 - Val accuracy 0.8351063829787234\n",
            "\n",
            "\n",
            "  5.2. Eval on the dev set..."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2302: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Acc.: 88.83\n",
            "\n",
            "\n",
            "Completed 5 runs.\n",
            "Dev accs: [86.97, 86.44, 86.7, 87.23, 88.83]\n",
            "Test accs: [-1, -1, -1, -1, -1]\n",
            "\n",
            "Mean Dev Acc.: 87.23 (0.84)\n",
            "Mean Test Acc.: -1.00 (0.00)\n",
            "\n",
            "Exec time: 892.20 s. ( 178 per run )\n"
          ]
        }
      ],
      "source": [
        "# Execute model\n",
        "\n",
        "def set_reproducible():\n",
        "    # The below is necessary to have reproducible behavior.\n",
        "    import random as rn\n",
        "    import os\n",
        "    os.environ['PYTHONHASHSEED'] = '0'\n",
        "    # The below is necessary for starting Numpy generated random numbers\n",
        "    # in a well-defined initial state.\n",
        "    np.random.seed(17)\n",
        "    # The below is necessary for starting core Python generated random numbers\n",
        "    # in a well-defined state.\n",
        "    rn.seed(12345)\n",
        "\n",
        "\n",
        "\n",
        "def load_label_output(filename):\n",
        "    with open(filename, 'r', encoding='UTF-8') as f:\n",
        "        return [line.strip().split(\"\\t\")[0] for line in f if line.strip()]\n",
        "\n",
        "\n",
        "\n",
        "def eval_list(glabels, slabels):\n",
        "    if (len(glabels) != len(slabels)):\n",
        "        print(\"\\nWARNING: label count in system output (%d) is different from gold label count (%d)\\n\" % (\n",
        "        len(slabels), len(glabels)))\n",
        "    n = min(len(slabels), len(glabels))\n",
        "    incorrect_count = 0\n",
        "    for i in range(n):\n",
        "        if slabels[i] != glabels[i]: incorrect_count += 1\n",
        "    acc = (n - incorrect_count) / n\n",
        "    return acc*100\n",
        "\n",
        "\n",
        "\n",
        "def train_and_eval(classifier, trainfile, devfile, testfile, run_id, device):\n",
        "    print(f\"\\nRUN: {run_id}\")\n",
        "    print(\"  %s.1. Training the classifier...\" % str(run_id))\n",
        "    classifier.train(trainfile, devfile, device)\n",
        "    print()\n",
        "    print(\"  %s.2. Eval on the dev set...\" % str(run_id), end=\"\")\n",
        "    slabels = classifier.predict(devfile, device)\n",
        "    glabels = load_label_output(devfile)\n",
        "    devacc = eval_list(glabels, slabels)\n",
        "    print(\" Acc.: %.2f\" % devacc)\n",
        "    testacc = -1\n",
        "    if testfile is not None:\n",
        "        # Evaluation on the test data\n",
        "        print(\"  %s.3. Eval on the test set...\" % str(run_id), end=\"\")\n",
        "        slabels = classifier.predict(testfile)\n",
        "        glabels = load_label_output(testfile)\n",
        "        testacc = eval_list(glabels, slabels)\n",
        "        print(\" Acc.: %.2f\" % testacc)\n",
        "    print()\n",
        "    return (devacc, testacc)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device_name = 'cuda'\n",
        "    device = torch.device(device_name)\n",
        "    n_runs = 5 #args.n_runs\n",
        "    set_reproducible()\n",
        "    datadir = \"../data/\"\n",
        "    trainfile =  datadir + \"traindata.csv\"\n",
        "    print(trainfile)\n",
        "    devfile =  datadir + \"devdata.csv\"\n",
        "    testfile = None\n",
        "    # testfile = datadir + \"testdata.csv\"\n",
        "\n",
        "    # Runs\n",
        "    start_time = time.perf_counter()\n",
        "    devaccs = []\n",
        "    testaccs = []\n",
        "    for i in range(1, n_runs+1):\n",
        "        classifier =  Classifier()\n",
        "        devacc, testacc = train_and_eval(classifier, trainfile, devfile, testfile, i, device)\n",
        "        devaccs.append(np.round(devacc,2))\n",
        "        testaccs.append(np.round(testacc,2))\n",
        "    print('\\nCompleted %d runs.' % n_runs)\n",
        "    total_exec_time = (time.perf_counter() - start_time)\n",
        "    print(\"Dev accs:\", devaccs)\n",
        "    print(\"Test accs:\", testaccs)\n",
        "    print()\n",
        "    print(\"Mean Dev Acc.: %.2f (%.2f)\" % (np.mean(devaccs), np.std(devaccs)))\n",
        "    print(\"Mean Test Acc.: %.2f (%.2f)\" % (np.mean(testaccs), np.std(testaccs)))\n",
        "    print(\"\\nExec time: %.2f s. ( %d per run )\" % (total_exec_time, total_exec_time / n_runs))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPBlEX1uw7/u+iLiGAmVx3p",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}